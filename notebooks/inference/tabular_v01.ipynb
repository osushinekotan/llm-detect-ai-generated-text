{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import DefaultDict\n",
    "\n",
    "import cupy as cp\n",
    "import hydra\n",
    "import imker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from hydra.utils import instantiate\n",
    "from imker.types import ArrayLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=004-tabular\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "\n",
    "logger.info(f\"start {OVERRIDES} ðŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(INPUT_DIR / \"test_essays.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(imker.BasePreProcessor):\n",
    "    def __init__(self):\n",
    "        self.text_cleansing_task = instantiate(CFG.imker.tasks.text_cleansing_task)\n",
    "        self.tfidf_vectorize_task_01 = instantiate(CFG.imker.tasks.tfidf_vectorize_task_01)\n",
    "        self.count_vectorize_task_01 = instantiate(CFG.imker.tasks.count_vectorize_task_01)\n",
    "\n",
    "        self.svd_decompose_task_01_tfidf_01 = instantiate(CFG.imker.tasks.svd_decompose_task_01)\n",
    "        self.svd_decompose_task_01_count_01 = instantiate(CFG.imker.tasks.svd_decompose_task_01)\n",
    "\n",
    "    @staticmethod\n",
    "    def ngram_range_to_tuple(cfg):\n",
    "        cfg.config.init_params = tuple(cfg.config.init_params)\n",
    "        return cfg\n",
    "\n",
    "    def to_dataframe(self, X, feature_name=\"\"):\n",
    "        return pd.DataFrame(X, columns=[f\"f_{feature_name}_{i:03}\" for i in range(X.shape[1])])\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        cleansed_texts = self.text_cleansing_task(X[\"text\"])\n",
    "\n",
    "        # tfidf\n",
    "        vecs = cp.asnumpy(self.tfidf_vectorize_task_01(pd.Series(cleansed_texts)).toarray())\n",
    "        vecs = self.svd_decompose_task_01_tfidf_01(vecs)\n",
    "        x_tfidf_vecs = self.to_dataframe(vecs, feature_name=\"tfidf_svd\")\n",
    "\n",
    "        # count\n",
    "        vecs = cp.asnumpy(self.tfidf_vectorize_task_01(pd.Series(cleansed_texts)).toarray())\n",
    "        vecs = self.svd_decompose_task_01_tfidf_01(vecs)\n",
    "        x_count_vecs = self.to_dataframe(vecs, feature_name=\"tfidf_svd\")\n",
    "\n",
    "        x_out = pd.concat([x_tfidf_vecs, x_count_vecs], axis=1)\n",
    "        y_out = y\n",
    "        return x_out, y_out\n",
    "\n",
    "\n",
    "class Splitter(imker.BaseSplitter):\n",
    "    def __init__(self):\n",
    "        self.splitter = imker.Task(\n",
    "            imker.TaskConfig(\n",
    "                task=hydra.utils.get_class(CFG.cv._target_),\n",
    "                init_params={k: v for k, v in CFG.cv.items() if k != \"_target_\"},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.splitter.get_n_splits()\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.splitter(X, y)\n",
    "\n",
    "\n",
    "class Classifier(imker.BaseModel):\n",
    "    def __init__(self):\n",
    "        self.knn_01 = instantiate(CFG.imker.tasks.knn_classifier_task_01)\n",
    "\n",
    "    def forward(self, X, y=None, proba=False):\n",
    "        feature_columns = [c for c in X.columns if c.startswith(\"f_\")]\n",
    "        return {\n",
    "            \"knn_01\": self.knn_01(X[feature_columns], y, proba=proba),\n",
    "        }\n",
    "\n",
    "\n",
    "class Scorer(imker.BaseScorer):\n",
    "    def calc_metrics(self, y_true: ArrayLike, y_pred: dict[str, ArrayLike]) -> pd.Series:\n",
    "        _results: DefaultDict[str, dict] = defaultdict(dict)\n",
    "        results = dict()\n",
    "\n",
    "        for model, pred in y_pred.items():\n",
    "            if np.ndim(pred) == 2:\n",
    "                pred = pred[:, 1]\n",
    "\n",
    "            for criteria in self.metrics:\n",
    "                _results[model][criteria.__name__] = criteria(y_true, pred)\n",
    "            results[model] = pd.Series(_results[model])\n",
    "        return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imker.Pipeline(\n",
    "    repo_dir=CFG.paths.output_dir,\n",
    "    exp_name=CFG.experiment_name,\n",
    "    pipeline_name=CFG.meta.competition,\n",
    "    preprocessor=Preprocessor,\n",
    "    splitter=Splitter,\n",
    "    model=Classifier,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = pipe.inference(X_test=test_df, proba=True)\n",
    "test_predictions = test_preds.knn_01[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "submission_df[\"generated\"] = test_predictions\n",
    "\n",
    "submission_df.to_csv(Path(CFG.paths.submission_dir) / \"submission.csv\", index=False)\n",
    "submission_df"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
