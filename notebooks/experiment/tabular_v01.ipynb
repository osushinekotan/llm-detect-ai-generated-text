{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import imker\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import wandb\n",
    "import hydra\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from hydra.utils import instantiate\n",
    "from kaggle import KaggleApi\n",
    "from lightning import seed_everything\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from src.utils.metrics import opt_acc_score, opt_f1_score\n",
    "from src.utils.kaggle_utils import download_kaggle_competition_dataset, download_kaggle_datasets\n",
    "from collections import defaultdict\n",
    "from typing import DefaultDict\n",
    "from imker.types import ArrayLike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=004-tabular\").split(\",\")\n",
    "WANDB_KEY = os.getenv(\"WANDB_KEY\", None)  # input your wandb key as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "KAGGLE_CLIENT = KaggleApi()\n",
    "KAGGLE_CLIENT.authenticate()\n",
    "\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "\n",
    "logger.info(f\"start {OVERRIDES} ðŸš€\")\n",
    "seed_everything(CFG.seed)\n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "nltk.data.path.append(CFG.paths.output_dir)\n",
    "nltk.download(\"punkt\", download_dir=CFG.paths.output_dir)\n",
    "\n",
    "os.environ[\"CUML_LOG_LEVEL\"] = \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_kaggle_competition_dataset(\n",
    "    client=KAGGLE_CLIENT,\n",
    "    competition=CFG.meta.competition,\n",
    "    out_dir=Path(CFG.paths.input_dir),\n",
    ")\n",
    "\n",
    "download_kaggle_datasets(\n",
    "    client=KAGGLE_CLIENT,\n",
    "    datasets=CFG.kaggle.external_datasets,\n",
    "    out_dir=INPUT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(INPUT_DIR / \"thedrcat/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n",
    "\n",
    "if CFG.debug:\n",
    "    train_df = train_df.sample(100, random_state=CFG.seed).reset_index(drop=True)\n",
    "    if \"debug\" not in CFG.lightning.logger.wandb.group:\n",
    "        CFG.lightning.logger.wandb.group = CFG.experiment_name + \"_debug\"\n",
    "\n",
    "logger.debug(f\"train shape : {train_df.shape}\")\n",
    "logger.debug(f\"train generated label : {train_df['label'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(imker.BasePreProcessor):\n",
    "    def __init__(self):\n",
    "        self.extract_raw_features_task = instantiate(CFG.imker.tasks.extract_raw_features_task)\n",
    "        self.text_cleansing_task = instantiate(CFG.imker.tasks.text_cleansing_task)\n",
    "        self.extract_tfidf_features_task = instantiate(CFG.imker.tasks.extract_tfidf_features_task)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        x_raw_features = self.extract_raw_features_task(X)\n",
    "        cleansed_texts = self.text_cleansing_task(X)\n",
    "        x_cleansed_texts = self.extract_tfidf_features_task(cleansed_texts)\n",
    "\n",
    "        x_out = pd.concat([x_raw_features, x_cleansed_texts], axis=1)\n",
    "        y_out = y\n",
    "        return x_out, y_out\n",
    "\n",
    "\n",
    "class Splitter(imker.BaseSplitter):\n",
    "    def __init__(self):\n",
    "        self.splitter = imker.Task(\n",
    "            imker.TaskConfig(\n",
    "                task=hydra.utils.get_class(CFG.cv._target_),\n",
    "                init_params={k: v for k, v in CFG.cv.items() if k != \"_target_\"},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.splitter.get_n_splits()\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.splitter(X, y)\n",
    "\n",
    "\n",
    "class Classifier(imker.BaseModel):\n",
    "    def __init__(self):\n",
    "        self.lr = instantiate(CFG.imker.tasks.logistic_regression_task)\n",
    "        self.knn = instantiate(CFG.imker.tasks.knn_classifier_task)\n",
    "\n",
    "    def forward(self, X, y=None, proba=False):\n",
    "        return {\n",
    "            \"lr\": self.lr(X, y, proba=proba),\n",
    "            \"knn\": self.knn(X, y, proba=proba),\n",
    "        }\n",
    "\n",
    "\n",
    "class Scorer(imker.BaseScorer):\n",
    "    def calc_metrics(self, y_true: ArrayLike, y_pred: dict[str, ArrayLike]) -> pd.Series:\n",
    "        _results: DefaultDict[str, dict] = defaultdict(dict)\n",
    "        results = dict()\n",
    "\n",
    "        for model, pred in y_pred.items():\n",
    "            if np.ndim(pred) == 2:\n",
    "                pred = pred[:, 1]\n",
    "\n",
    "            for criteria in self.metrics:\n",
    "                _results[model][criteria.__name__] = criteria(y_true, pred)\n",
    "            results[model] = pd.Series(_results[model])\n",
    "        return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imker.Pipeline(\n",
    "    repo_dir=CFG.paths.output_dir,\n",
    "    exp_name=CFG.experiment_name,\n",
    "    pipeline_name=CFG.meta.competition,\n",
    ")\n",
    "pipe.set_preprocessor(Preprocessor)\n",
    "pipe.set_splitter(Splitter)\n",
    "pipe.set_model(Classifier)\n",
    "pipe.set_metrics(\n",
    "    metrics=[\n",
    "        roc_auc_score,\n",
    "        log_loss,\n",
    "        opt_acc_score,\n",
    "        opt_f1_score,\n",
    "    ],\n",
    "    scorer=Scorer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.train(X=train_df, y=train_df[\"label\"])\n",
    "val_preds = pipe.validate(X=train_df, y=train_df[\"label\"], proba=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_scores()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
