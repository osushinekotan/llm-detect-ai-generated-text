{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "from typing import DefaultDict\n",
    "\n",
    "import cupy as cp\n",
    "import hydra\n",
    "import imker\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import wandb\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from hydra.utils import instantiate\n",
    "from imker.types import ArrayLike\n",
    "from kaggle import KaggleApi\n",
    "from lightning import seed_everything\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "\n",
    "from src.utils.kaggle_utils import download_kaggle_competition_dataset, download_kaggle_datasets\n",
    "from src.utils.metrics import opt_acc_score, opt_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=004-tabular\").split(\",\")\n",
    "WANDB_KEY = os.getenv(\"WANDB_KEY\", None)  # input your wandb key as environment variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "KAGGLE_CLIENT = KaggleApi()\n",
    "KAGGLE_CLIENT.authenticate()\n",
    "\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "\n",
    "logger.info(f\"start {OVERRIDES} ðŸš€\")\n",
    "seed_everything(CFG.seed)\n",
    "wandb.login(key=WANDB_KEY)\n",
    "\n",
    "nltk.data.path.append(CFG.paths.output_dir)\n",
    "nltk.download(\"punkt\", download_dir=CFG.paths.output_dir)\n",
    "\n",
    "os.environ[\"CUML_LOG_LEVEL\"] = \"error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_kaggle_competition_dataset(\n",
    "    client=KAGGLE_CLIENT,\n",
    "    competition=CFG.meta.competition,\n",
    "    out_dir=Path(CFG.paths.input_dir),\n",
    ")\n",
    "\n",
    "download_kaggle_datasets(\n",
    "    client=KAGGLE_CLIENT,\n",
    "    datasets=CFG.kaggle.external_datasets,\n",
    "    out_dir=INPUT_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(INPUT_DIR / \"thedrcat/daigt-v2-train-dataset/train_v2_drcat_02.csv\")\n",
    "\n",
    "if CFG.debug:\n",
    "    train_df = train_df.sample(500, random_state=CFG.seed).reset_index(drop=True)\n",
    "    if \"debug\" not in CFG.lightning.logger.wandb.group:\n",
    "        CFG.lightning.logger.wandb.group = CFG.experiment_name + \"_debug\"\n",
    "\n",
    "logger.debug(f\"train shape : {train_df.shape}\")\n",
    "logger.debug(f\"train generated label : {train_df['label'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor(imker.BasePreProcessor):\n",
    "    def __init__(self):\n",
    "        self.text_cleansing_task = instantiate(CFG.imker.tasks.text_cleansing_task)\n",
    "        self.tfidf_vectorize_task_01 = instantiate(CFG.imker.tasks.tfidf_vectorize_task_01)\n",
    "        self.count_vectorize_task_01 = instantiate(CFG.imker.tasks.count_vectorize_task_01)\n",
    "\n",
    "        self.svd_decompose_task_01_tfidf_01 = instantiate(CFG.imker.tasks.svd_decompose_task_01)\n",
    "        self.svd_decompose_task_01_count_01 = instantiate(CFG.imker.tasks.svd_decompose_task_01)\n",
    "\n",
    "    @staticmethod\n",
    "    def ngram_range_to_tuple(cfg):\n",
    "        cfg.config.init_params = tuple(cfg.config.init_params)\n",
    "        return cfg\n",
    "\n",
    "    def to_dataframe(self, X, feature_name=\"\"):\n",
    "        return pd.DataFrame(X, columns=[f\"f_{feature_name}_{i:03}\" for i in range(X.shape[1])])\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "        cleansed_texts = self.text_cleansing_task(X[\"text\"])\n",
    "\n",
    "        # tfidf\n",
    "        vecs = cp.asnumpy(self.tfidf_vectorize_task_01(pd.Series(cleansed_texts)).toarray())\n",
    "        vecs = self.svd_decompose_task_01_tfidf_01(vecs)\n",
    "        x_tfidf_vecs = self.to_dataframe(vecs, feature_name=\"tfidf_svd\")\n",
    "\n",
    "        # count\n",
    "        vecs = cp.asnumpy(self.tfidf_vectorize_task_01(pd.Series(cleansed_texts)).toarray())\n",
    "        vecs = self.svd_decompose_task_01_tfidf_01(vecs)\n",
    "        x_count_vecs = self.to_dataframe(vecs, feature_name=\"tfidf_svd\")\n",
    "\n",
    "        x_out = pd.concat([x_tfidf_vecs, x_count_vecs], axis=1)\n",
    "        y_out = y\n",
    "        return x_out, y_out\n",
    "\n",
    "\n",
    "class Splitter(imker.BaseSplitter):\n",
    "    def __init__(self):\n",
    "        self.splitter = imker.Task(\n",
    "            imker.TaskConfig(\n",
    "                task=hydra.utils.get_class(CFG.cv._target_),\n",
    "                init_params={k: v for k, v in CFG.cv.items() if k != \"_target_\"},\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.splitter.get_n_splits()\n",
    "\n",
    "    def split(self, X, y=None):\n",
    "        return self.splitter(X, y)\n",
    "\n",
    "\n",
    "class Classifier(imker.BaseModel):\n",
    "    def __init__(self):\n",
    "        self.knn_01 = instantiate(CFG.imker.tasks.knn_classifier_task_01)\n",
    "\n",
    "    def forward(self, X, y=None, proba=False):\n",
    "        feature_columns = [c for c in X.columns if c.startswith(\"f_\")]\n",
    "        return {\n",
    "            \"knn_01\": self.knn_01(X[feature_columns], y, proba=proba),\n",
    "        }\n",
    "\n",
    "\n",
    "class Scorer(imker.BaseScorer):\n",
    "    def calc_metrics(self, y_true: ArrayLike, y_pred: dict[str, ArrayLike]) -> pd.Series:\n",
    "        _results: DefaultDict[str, dict] = defaultdict(dict)\n",
    "        results = dict()\n",
    "\n",
    "        for model, pred in y_pred.items():\n",
    "            if np.ndim(pred) == 2:\n",
    "                pred = pred[:, 1]\n",
    "\n",
    "            for criteria in self.metrics:\n",
    "                _results[model][criteria.__name__] = criteria(y_true, pred)\n",
    "            results[model] = pd.Series(_results[model])\n",
    "        return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = imker.Pipeline(\n",
    "    repo_dir=CFG.paths.output_dir,\n",
    "    exp_name=CFG.experiment_name,\n",
    "    pipeline_name=CFG.meta.competition,\n",
    ")\n",
    "pipe.set_preprocessor(Preprocessor)\n",
    "pipe.set_splitter(Splitter)\n",
    "pipe.set_model(Classifier)\n",
    "pipe.set_metrics(\n",
    "    metrics=[\n",
    "        roc_auc_score,\n",
    "        log_loss,\n",
    "        opt_acc_score,\n",
    "        opt_f1_score,\n",
    "    ],\n",
    "    scorer=Scorer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.train(X=train_df, y=train_df[\"label\"])\n",
    "pipe.validate(X=train_df, y=train_df[\"label\"], proba=True, calc_metrics=True)\n",
    "scores_df = pipe.get_scores()\n",
    "scores_df.to_csv(CFG.paths.output_dir / \"scores.csv\", index=False)\n",
    "display(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
