# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /lightning/data: default
  - override /lightning/model: default
  - override /lightning/callbacks: default
  - override /lightning/trainer: default
  - override /lightning/logger: default
  - override /cv: stratified_kfold

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

experiment_name: ${hydra:job.override_dirname}
notebook: v01

debug: true
tags: ["debug", "fine-tuning", "baseline"]
description: "debugging baseline"
seed: 42

train: true
n_splits: 5 # total folds
train_folds: [0, 1] # train folds

model_name: microsoft/deberta-v3-small
ckpt_path: null # example: /path/to/{fold}/model.ckpt

lightning:
  model:
    net:
      gradient_checkpointing_enable: true
  data:
    lt_datamodule:
      batch_size: 8
    train_dataset:
      max_length: 128
    val_dataset:
      max_length: 8
    test_dataset:
      max_length: 128
    test_dataloader:
      batch_size: 32
  trainer:
    max_epochs: 4
    precision: 16
  logger:
    wandb:
      tags: ${tags}
      group: ${experiment_name}
