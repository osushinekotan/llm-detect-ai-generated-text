# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /lightning/data: default
  - override /lightning/model: default
  - override /lightning/callbacks: default
  - override /lightning/trainer: default
  - override /lightning/logger: default
  - override /cv: stratified_kfold

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

experiment_name: ${hydra:job.override_dirname}
notebook: finetune_v01

tags: ["fine-tuning", "baseline"]
description: ""

debug: false
train: true

n_splits: 10 # total folds
train_folds: [0] # train folds

model_name: microsoft/deberta-v3-large
ckpt_path: null # example: /path/to/{fold}/model.ckpt

lightning:
  model:
    net:
      gradient_checkpointing_enable: true
  data:
    lt_datamodule:
      batch_size: 4
    train_dataset:
      max_length: 512
    val_dataset:
      max_length: 1024
    test_dataset:
      max_length: 1024
    test_dataloader:
      batch_size: 16
  trainer:
    max_epochs: 4
    precision: 16
  logger:
    wandb:
      tags: ${tags}
      group: ${experiment_name}
